{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbJtG1gTbA7HqrCisYbGHs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Russel-hunho/DeepLearning/blob/main/pytorch_library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. 미니 배치와 데이터 로드\n",
        "https://wikidocs.net/55580\n",
        "\n",
        "-. 데이터 로드법\n",
        "\n",
        "-. 미니 배치 경사 하강법(Minibatch Gradient Descent)"
      ],
      "metadata": {
        "id": "w1ipT2YM5gZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1. 미니 배치"
      ],
      "metadata": {
        "id": "hljqsWQ_8NZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "미니 배치(Mini Batch): 전체 Training Data를 작은 단위로 나누어서 학습하는 개념\n",
        "\n",
        "> 기본적으로, 학습 Input Data는 Data 여러개를 묶어 행렬로 만들어, Batch 단위로 학습에 사용된다. (= 배치 경사 하강법)\n",
        "\n",
        "> Batch의 크기가 크면 클수록, 가중치 값이 최적값에 수렴하는 과정이 매우 안정적이지만, 계산량이 너무 많이 듬\n",
        "\n",
        "> 미니 배치로 나누어 진행하면 훈련 속도가 빨라진다!"
      ],
      "metadata": {
        "id": "QY3kOpGK556b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Iteration, Epoch, Batch Size***\n",
        "\n",
        "Total data가 2000개일 때,\n",
        "이를 Mini Batch 10개로 나누면,\n",
        "> Iteration = 10\n",
        "> Batch Size = 200\n",
        "\n",
        "**Epoch** = 총 학습 수 = 모든 data가 학습에 사용된 횟수\n",
        "\n",
        "<->\n",
        "\n",
        "**Iteration** = 한 Epoch에서 진행되는 학습의 수 = Mini Batch의 수"
      ],
      "metadata": {
        "id": "_DFcmzcm6Isn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. 데이터 로드"
      ],
      "metadata": {
        "id": "DDbeXiB38QBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋(Dataset), 데이터로더(DataLoader)가 제공된다\n",
        "\n",
        "데이터로더: 데이터셋의 학습 방법을 결정\n",
        "\n",
        "    batch_size = 2; 미니 배치의 크기는?\n",
        "    shuffle = True; 미니 배치 data를 섞어 진행할건가?"
      ],
      "metadata": {
        "id": "no5sniht8VHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 미니 배치 학습\n",
        "\n",
        "> 데이터 셔플(shuffle)\n",
        "\n",
        "> 병렬처리"
      ],
      "metadata": {
        "id": "Yyoh5Jmk8dQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Rx5bF-_L7XWs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
        "from torch.utils.data import DataLoader # 데이터로더"
      ],
      "metadata": {
        "id": "YRKXs0V47XAv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' 사용법 예시 '''\n",
        "\n",
        "# 데이터 예시\n",
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  90], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
        "\n",
        "# TensorDataset으로 저장하기\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "\n",
        "# DataLoader 정의\n",
        "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)\n",
        "  # Batch Size는 2의 지수승으로 정의한다: CPU, GPU의 용량을 최적으로 사용할 수 있음!"
      ],
      "metadata": {
        "id": "7QtpPqNh5fmB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zeozgvQ5SJt",
        "outputId": "31790015-db9e-47e5-af70-b6cf5b972908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/100 Batch 3/3 Cost: 32791.722656\n",
            "Epoch   10/100 Batch 3/3 Cost: 3888.384277\n",
            "Epoch   20/100 Batch 3/3 Cost: 181.972015\n",
            "Epoch   30/100 Batch 3/3 Cost: 21.030962\n",
            "Epoch   40/100 Batch 3/3 Cost: 2.584492\n",
            "Epoch   50/100 Batch 3/3 Cost: 0.832111\n",
            "Epoch   60/100 Batch 3/3 Cost: 3.371171\n",
            "Epoch   70/100 Batch 3/3 Cost: 0.263154\n",
            "Epoch   80/100 Batch 3/3 Cost: 0.261747\n",
            "Epoch   90/100 Batch 3/3 Cost: 3.131727\n",
            "Epoch  100/100 Batch 3/3 Cost: 0.201195\n"
          ]
        }
      ],
      "source": [
        "# 선형 다중 회귀모델로 적용 연습\n",
        "\n",
        "model = nn.Linear(3,1) # 3변수->1변수\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-6)\n",
        "\n",
        "nb_epochs = 100\n",
        "for epoch in range(nb_epochs+1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    # Batch Size: 2로 설정, x_train의 data는 5개\n",
        "    # -> 5개중 (2,2,1)로 Batch를 만들어 학습을 진행한다!\n",
        "    \n",
        "    #print(batch_idx)\n",
        "    #print(samples)\n",
        "    \n",
        "    # 미니 bacth data를 train 값으로 설정\n",
        "    x_train, y_train = samples\n",
        "\n",
        "    # H(x)\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # Cost\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    # Opti\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  \n",
        "    '''print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()\n",
        "        ))'''\n",
        "  if epoch%10 == 0:\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. 커스텀 데이터셋(Custom Dataset)\n",
        "https://wikidocs.net/57165"
      ],
      "metadata": {
        "id": "9_L_KSr9H38S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.utils.data.Dataset을 상속받는 Class를 만들어 커스터마이징 하기도 한다!"
      ],
      "metadata": {
        "id": "UdDPCadfH8ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 기본 틀'''\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  # 생성자; 데이터셋의 전처리 부분\n",
        "  def __init__(self):\n",
        "    return None\n",
        "\n",
        "  # 데이터셋의 길이: 총 샘플의 수를 적어주는 부분\n",
        "  def __len__(self):\n",
        "    return None\n",
        "  \n",
        "  # 데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "  def __getitem__(self, idx):\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "WXRTq1D1H8C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Custom Dataset으로 선형 회귀 구현하기 '''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "## Custom Dataset Class 구현\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = [[73,80,75],\n",
        "                   [93,88,93],\n",
        "                   [89,91,90],\n",
        "                   [96,98,100],\n",
        "                   [73,66,70]]\n",
        "    self.y_data = [[152],[185],[180],[196],[142]]\n",
        "\n",
        "  # 총 데이터 개수 리턴\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "  \n",
        "  # index(idx)값을 받아, 해당하는 Data를 추출하여 보여준다\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    return x,y\n",
        "\n",
        "\n",
        "## dataset 정의, Dataloader 설정 (mini batch)\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)\n",
        "\n",
        "\n",
        "## model 정의\n",
        "model = nn.Linear(3,1) # 3변수 -> 1변수\n",
        "\n",
        "## Optimizer 정의 : SGD\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
        "\n",
        "## 학습\n",
        "nb_epochs = 50\n",
        "for epoch in range(nb_epochs+1):\n",
        "  # 미니배치 실행\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    x_train, y_train = samples\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    # 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  #epoch마다 출력\n",
        "  print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "      epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
        "      cost.item()\n",
        "      ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndp6kXoFItmX",
        "outputId": "bc2b6847-6ee0-496e-f47d-ef28cd0996c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/50 Batch 3/3 Cost: 1018.360352\n",
            "Epoch    1/50 Batch 3/3 Cost: 7.520262\n",
            "Epoch    2/50 Batch 3/3 Cost: 0.151220\n",
            "Epoch    3/50 Batch 3/3 Cost: 1.551571\n",
            "Epoch    4/50 Batch 3/3 Cost: 7.717453\n",
            "Epoch    5/50 Batch 3/3 Cost: 9.120426\n",
            "Epoch    6/50 Batch 3/3 Cost: 1.799313\n",
            "Epoch    7/50 Batch 3/3 Cost: 1.891422\n",
            "Epoch    8/50 Batch 3/3 Cost: 8.972920\n",
            "Epoch    9/50 Batch 3/3 Cost: 4.729444\n",
            "Epoch   10/50 Batch 3/3 Cost: 1.716973\n",
            "Epoch   11/50 Batch 3/3 Cost: 10.554765\n",
            "Epoch   12/50 Batch 3/3 Cost: 9.122454\n",
            "Epoch   13/50 Batch 3/3 Cost: 7.356381\n",
            "Epoch   14/50 Batch 3/3 Cost: 3.339810\n",
            "Epoch   15/50 Batch 3/3 Cost: 8.372835\n",
            "Epoch   16/50 Batch 3/3 Cost: 3.243308\n",
            "Epoch   17/50 Batch 3/3 Cost: 10.160448\n",
            "Epoch   18/50 Batch 3/3 Cost: 4.426319\n",
            "Epoch   19/50 Batch 3/3 Cost: 1.666918\n",
            "Epoch   20/50 Batch 3/3 Cost: 1.848938\n",
            "Epoch   21/50 Batch 3/3 Cost: 7.348355\n",
            "Epoch   22/50 Batch 3/3 Cost: 5.405469\n",
            "Epoch   23/50 Batch 3/3 Cost: 3.919416\n",
            "Epoch   24/50 Batch 3/3 Cost: 3.448372\n",
            "Epoch   25/50 Batch 3/3 Cost: 3.444067\n",
            "Epoch   26/50 Batch 3/3 Cost: 1.414400\n",
            "Epoch   27/50 Batch 3/3 Cost: 2.808754\n",
            "Epoch   28/50 Batch 3/3 Cost: 2.001040\n",
            "Epoch   29/50 Batch 3/3 Cost: 0.708648\n",
            "Epoch   30/50 Batch 3/3 Cost: 1.358952\n",
            "Epoch   31/50 Batch 3/3 Cost: 1.465446\n",
            "Epoch   32/50 Batch 3/3 Cost: 2.423985\n",
            "Epoch   33/50 Batch 3/3 Cost: 5.196794\n",
            "Epoch   34/50 Batch 3/3 Cost: 3.840012\n",
            "Epoch   35/50 Batch 3/3 Cost: 4.598556\n",
            "Epoch   36/50 Batch 3/3 Cost: 7.892742\n",
            "Epoch   37/50 Batch 3/3 Cost: 3.186456\n",
            "Epoch   38/50 Batch 3/3 Cost: 8.042888\n",
            "Epoch   39/50 Batch 3/3 Cost: 3.029372\n",
            "Epoch   40/50 Batch 3/3 Cost: 1.346069\n",
            "Epoch   41/50 Batch 3/3 Cost: 3.075491\n",
            "Epoch   42/50 Batch 3/3 Cost: 5.405469\n",
            "Epoch   43/50 Batch 3/3 Cost: 2.978121\n",
            "Epoch   44/50 Batch 3/3 Cost: 1.940209\n",
            "Epoch   45/50 Batch 3/3 Cost: 4.165246\n",
            "Epoch   46/50 Batch 3/3 Cost: 6.782903\n",
            "Epoch   47/50 Batch 3/3 Cost: 2.762810\n",
            "Epoch   48/50 Batch 3/3 Cost: 9.795976\n",
            "Epoch   49/50 Batch 3/3 Cost: 3.993411\n",
            "Epoch   50/50 Batch 3/3 Cost: 9.754470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 확인\n",
        "\n",
        "new_var = torch.FloatTensor([[73,80,75]])\n",
        "pred_y = model(new_var)\n",
        "print(\"훈련 후 입력이 73,80,75일 때의 예측값: \", float(pred_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y27OZmfwLE4g",
        "outputId": "fea3b4a0-4b58-472f-dbb3-afab033b136a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 후 입력이 73,80,75일 때의 예측값:  154.08950805664062\n"
          ]
        }
      ]
    }
  ]
}