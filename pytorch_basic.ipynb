{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJbecJtGypb+e/kANmu54I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Russel-hunho/DeepLearning/blob/main/pytorch_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(https://wikidocs.net/52415)\n",
        "# 파이토치 기본"
      ],
      "metadata": {
        "id": "gM_f_Mgv0gEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 파이토치 패키지"
      ],
      "metadata": {
        "id": "yEWaVK9f2GxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yAQ_6h_r0Bjp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "  # main\n",
        "import torch.autograd\n",
        "  # 자동 미분 기능\n",
        "import torch.nn\n",
        "  # 신경망 구축을 위한 다양한 데이터구조, 레이어 등이 정의됨\n",
        "    # RNN, LSTM 등의 레이어 / ReLU 등의 활성화함수 / MSELoss 등의 손실함수\n",
        "import torch.optim\n",
        "  # SGD(Stochastic Gradient Descent, 확률적 경사 하강법) 중심의, 파라미터 최적화 알고리즘 구현\n",
        "import torch.utils.data\n",
        "  # SGD의 반복연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수 포함\n",
        "import torch.onnx\n",
        "  # ONNX(Open Neural Network Exchange) 포맷.\n",
        "    # 모델을 export할 때 사용됨, 서로다른 딥 러닝 프레임워크 간 모델 공유 시에 사용\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 텐서 조작하기"
      ],
      "metadata": {
        "id": "VTYb72PA2KbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[example]\n",
        "1. 2D Tensor for Simple Setting\n",
        "\n",
        "  |t| = (Batch size, dim)\n",
        "2. 3D Tensor for 컴퓨터비전\n",
        "\n",
        "  |t| = (Batch size, width, height)\n",
        "3. 3D Tensor for 자연어 처리(NLP)\n",
        "\n",
        "  |t| = (Batch size, length, dim)\n",
        "        lenght = 문장길이\n",
        "        dim = 단어 벡터의 차원\n"
      ],
      "metadata": {
        "id": "dySYTy2-2wd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' NLP 3D Tensor 변환 이해하기 '''\n",
        "## 1차원 입력 raw data\n",
        "A = [\"나는 사과를 좋아해\", \"나는 바나나를 좋아해\", \"나는 사과를 싫어해\", \"나는 바나나를 싫어해\"]\n",
        "\n",
        "## 2D로 변환 for 단어 구분\n",
        "  # 4x3 2D Tensor\n",
        "B = [[\"나는\", \"사과를\", \"좋아해\"],\n",
        " [\"나는\", \"바나나를\", \"좋아해\"],\n",
        "  [\"나는\", \"사과를\", \"싫어해\"],\n",
        "  [\"나는\", \"바나나를\", \"싫어해\"]]\n",
        "\n",
        "## 각 단어를 vector로 변환하여, 3D로 변환\n",
        "'''\n",
        "  '나는' = [0.1, 0.2, 0.9]\n",
        "  '사과를' = [0.3, 0.5, 0.1]\n",
        "  '바나나를' = [0.3, 0.5, 0.2]\n",
        "  '좋아해' = [0.7, 0.6, 0.5]\n",
        "  '싫어해' = [0.5, 0.6, 0.7]\n",
        "'''\n",
        "C = [[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],\n",
        " [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],\n",
        " [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],\n",
        " [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]\n",
        "\n",
        "## Batch size=2 로, Data 구분\n",
        "Batch1 = [[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],\n",
        "         [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]]]\n",
        "Batch2 = [[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],\n",
        "         [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]\n"
      ],
      "metadata": {
        "id": "ibKJcmEb2KFz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Numpy로 Tensor 만들기 '''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 1D\n",
        "t = np.array([0.,1.,2.,3.,4.,5.,6.])\n",
        "print(t)\n",
        "print(\"Rank of t: \", t.ndim)\n",
        "print(\"Shape of t: \", t.shape, end=\"\\n\\n\")\n",
        "\n",
        "print(\"t[0] t[1] t[-1] = \", t[0], t[1], t[-1])\n",
        "  #slicing \n",
        "print('t[2:5] t[4:-1]  = ', t[2:5], t[4:-1])\n",
        "print('t[:2] t[3:]     = ', t[:2], t[3:], end=\"\\n\\n\\n\")\n",
        "\n",
        "# 2D\n",
        "t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
        "print(t)\n",
        "print('Rank  of t: ', t.ndim)\n",
        "print('Shape of t: ', t.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRmaP1pA3kA0",
        "outputId": "dc710d7c-42bb-43b5-c453-187142b97b57"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 2. 3. 4. 5. 6.]\n",
            "Rank of t:  1\n",
            "Shape of t:  (7,)\n",
            "\n",
            "t[0] t[1] t[-1] =  0.0 1.0 6.0\n",
            "t[2:5] t[4:-1]  =  [2. 3. 4.] [4. 5.]\n",
            "t[:2] t[3:]     =  [0. 1.] [3. 4. 5. 6.]\n",
            "\n",
            "\n",
            "[[ 1.  2.  3.]\n",
            " [ 4.  5.  6.]\n",
            " [ 7.  8.  9.]\n",
            " [10. 11. 12.]]\n",
            "Rank  of t:  2\n",
            "Shape of t:  (4, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' PyTorch로 Tensor 만들기 '''\n",
        "\n",
        "import torch\n",
        "\n",
        "## 1D\n",
        "t = torch.FloatTensor( [0., 1., 2., 3., 4., 5., 6. ] )\n",
        "print(t)\n",
        "print(t.dim())  # rank. 즉, 차원\n",
        "print(t.shape)  # shape\n",
        "print(t.size()) # size = shape\n",
        "print(type(t), end=\"\\n\\n\")\n",
        "\n",
        "print(t[0], t[1], t[-1])  # 인덱스로 접근\n",
        "print(t[2:5], t[4:-1])    # 슬라이싱\n",
        "print(t[:2], t[3:])       # 슬라이싱\n",
        "\n",
        "## 2D\n",
        "t = torch.FloatTensor([[1., 2., 3.],\n",
        "                       [4., 5., 6.],\n",
        "                       [7., 8., 9.],\n",
        "                       [10., 11., 12.]])\n",
        "print(t)\n",
        "print(t.dim())  # rank. 즉, 차원\n",
        "print(t.size(), end = \"\\n\\n\") # shape\n",
        "\n",
        "print(t[:,1])\n",
        "print(t[:,1].size())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlJCoYVG6o-O",
        "outputId": "72ebeefa-007e-4e6d-f022-dbbffe46c967"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 2., 3., 4., 5., 6.])\n",
            "1\n",
            "torch.Size([7])\n",
            "torch.Size([7])\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "tensor(0.) tensor(1.) tensor(6.)\n",
            "tensor([2., 3., 4.]) tensor([4., 5.])\n",
            "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n",
            "tensor([[ 1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.],\n",
            "        [ 7.,  8.,  9.],\n",
            "        [10., 11., 12.]])\n",
            "2\n",
            "torch.Size([4, 3])\n",
            "\n",
            "tensor([ 2.,  5.,  8., 11.])\n",
            "torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''브로드캐스팅'''\n",
        "## 사칙연산을 위해 두 행렬의 크기를 자동으로 맞춰 연산\n",
        "\n",
        "## 1. 같은 크기의 두 tensor일 경우\n",
        "m1 = torch.FloatTensor( [[3,3]] )\n",
        "m2 = torch.FloatTensor( [[2,2]] )\n",
        "print(m1+m2, end = \"\\n\\n\")\n",
        "\n",
        "## 2. 다른 크기의 두 Tensor일 경우\n",
        "\n",
        "# 2.1. vector + scalar: scalar가 vector size에 맞게 복제됨\n",
        "m1 = torch.FloatTensor([[1, 2]])\n",
        "m2 = torch.FloatTensor([3]) # [3] -> [3, 3]\n",
        "print(m1+m2, end = \"\\n\\n\")\n",
        "\n",
        "# 2.2. 2 x 1 Vector + 1 x 2 Vector\n",
        "m1 = torch.FloatTensor([[1, 2]]) # [[1,2]] -> [[1,2],[1,2]]\n",
        "m2 = torch.FloatTensor([[3], [4]]) # [[3],[4]] -> [[3,3], [4,4]]\n",
        "print(m1+m2, end = \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3xBDwvn8ghF",
        "outputId": "3192418e-b431-4b48-d272-081b746d1da1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5., 5.]])\n",
            "\n",
            "tensor([[4., 5.]])\n",
            "\n",
            "tensor([[4., 5.],\n",
            "        [5., 6.]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' 행렬곱셈(matmul)과 곱셈(*)의 차이'''\n",
        "\n",
        "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
        "m2 = torch.FloatTensor([[1], [2]])\n",
        "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
        "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
        "print(\"\")\n",
        "\n",
        "## 행렬곱셈(matmul):\n",
        "print(m1.matmul(m2)) # 2 x 1\n",
        "print(\"\")\n",
        "\n",
        "## 곱셈(*):\n",
        "print(m1 * m2) # 2 x 2\n",
        "  # m2가 m1 size에 맞게 브로드캐스팅 된 후, 같은 위치의 값들끼리만 곱해짐\n",
        "print(m1.mul(m2)) # 같은 결과"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyi38Zkr8mbp",
        "outputId": "8f4aeb22-8297-4d5d-fb1b-2b6623c1b6da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Matrix 1:  torch.Size([2, 2])\n",
            "Shape of Matrix 2:  torch.Size([2, 1])\n",
            "\n",
            "tensor([[ 5.],\n",
            "        [11.]])\n",
            "\n",
            "tensor([[1., 2.],\n",
            "        [6., 8.]])\n",
            "tensor([[1., 2.],\n",
            "        [6., 8.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' 다양한 연산 '''\n",
        "\n",
        "## 평균\n",
        "t = torch.FloatTensor([1,2])\n",
        "print(t.mean())\n",
        "print(int(t.mean()), end=\"\\n\\n\")\n",
        "\n",
        "t = torch.FloatTensor([[1,2],[3,4]])\n",
        "print(t.mean()) # 전체 원소의 평균\n",
        "print(t.mean(dim=0)) # 첫번째 차원(행)을 제거, 각 열의 평균을 구함\n",
        "print(t.mean(dim=1)) # 두번째 차원(열)을 제거, 각 행의 평균을 구함\n",
        "print(t.mean(dim=-1)) # 마지막 차원(=두번째=열)을 제거, 각 행의 평균을 구함\n",
        "try: print(t.mean(dim=2))\n",
        "except:  print(\"Fail\")\n",
        "print()\n",
        "\n",
        "## 덧셈\n",
        "t = torch.FloatTensor([[1,2],[3,4]])\n",
        "print(t.sum()) # 전체 합\n",
        "print(t.sum(dim=0)) # 행 제거\n",
        "print(t.sum(dim=1)) # 열 제거\n",
        "print(t.sum(dim=-1)) # 열 제거\n",
        "print()\n",
        "\n",
        "## 최댓값(Max), 아그맥스(ArgMax)\n",
        "t =torch.FloatTensor( [[1,2],[3,4]] )\n",
        "print(t.max()) # 전체의 최댓값\n",
        "print(t.max(dim=0)) # 행 제거\n",
        "  # 1. \"각 열의 최댓값\"(max) + 2. \"각 열 최댓값의 위치(행 값)\"(ArgMax) 반환\n",
        "print(\"Max: \", t.max(dim=0)[0]) # 각 열의 최댓값만\n",
        "print(\"ArgMax: \", t.max(dim=0)[1]) # 각 열의 argmax만\n",
        "print(t.argmax()) #전체 최댓값의 위치. 0,1,2,3중 4가 위치한 3\n",
        "print(t.argmax(dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06eyJQ4QCwLB",
        "outputId": "63b4e14d-f536-41fb-cdc3-7ada4a148ee1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.5000)\n",
            "1\n",
            "\n",
            "tensor(2.5000)\n",
            "tensor([2., 3.])\n",
            "tensor([1.5000, 3.5000])\n",
            "tensor([1.5000, 3.5000])\n",
            "Fail\n",
            "\n",
            "tensor(10.)\n",
            "tensor([4., 6.])\n",
            "tensor([3., 7.])\n",
            "tensor([3., 7.])\n",
            "\n",
            "tensor(4.)\n",
            "torch.return_types.max(\n",
            "values=tensor([3., 4.]),\n",
            "indices=tensor([1, 1]))\n",
            "Max:  tensor([3., 4.])\n",
            "ArgMax:  tensor([1, 1])\n",
            "tensor(3)\n",
            "tensor([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P2EMfvldiWyF"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}